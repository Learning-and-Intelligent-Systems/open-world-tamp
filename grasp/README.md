# Grasping Installation and Interface Guide


## Installation

## Running locally
This code was teseted with Ubuntu 18.04 with GPUs and CUDA 10.1 installed.
Although these instructions work with docker, you can also just install the dependencies in the ./Dockerfile on a linux machine and it will also work. 

1. `git clone git@github.mit.edu:Learning-and-Intelligent-Systems/open-world-tamp.git`
2. `git checkout grasp_new`
3. `cd open-world-tamp`
4. `git submodule update --init --recursive`
5. `docker build -t <docker-repo-name> .`
6. `docker run <docker-repo-name>`
7. `docker exec -it <container-name> bash`
8. `sh dockerinstall.sh`


### Running on LIS cloud
The only GPUs I have access to are on LIS cloud so I execute the following commands to test on lis-cluster starting after step 5 from above

6. `docker push <docker-repo-name>`
7. `kubectl apply -f grasptest.yaml -n <cloud-username>`
8. `kubectl get pods -n <cloud-username>`

This command will list the pods in your namespace. Find the name of the pod you just deployed

9. `kubectl exec -it <pod-name> -c owt bash -n <cloud-username>`
8. `sh dockerinstall.sh`

If you want to restart a pod then execute

`kubectl delete job owt -n <cloud-username>`


NOTE: There are several references to my miniclient bucket which hosts some datasets, models, etc. If there is any issue with access to that let me know and I'll try to find a way to host those files publically on the bucket.


## Interface


Here are all of the files in the ./grasp directory and their current uses


### tabletop_grasp_gen.py
This file sets up the pybullet environment with an object and a pr2 robot. It then collects images and ground truth segmentations from the mounted camera and passes them to graspnet_interface to generate the grasps

### graspnet_interface.py
This provides helper functions to access graspnet which take in point clouds and generate, save, and score grasps. After generating grasps, they are saved in miniclient for later visualization and use

### demo_vis.py
Once you have generated a set of grasps for a particular pointcloud and stored them in the miniclient bucket, this file reads from the miniclient bucket, plots the 3d point cloud and grasps with grasp importance.

### render_grasps.py
This file sets up the pr2 robot and visualizes the grasps generated by graspnet in pybullet by rendering an actual gripper model with the correct transformations

### segmentation_to_grasp_pipeline.py
This file is mostly a copy of tabletop_grasp_gen except for instead of using the ground truth segmentation, it generates a segmentation using Detectron2 maskrcnn and removes statistical outliers from the segmented pointcloud before running the total pointcloud through graspnet.

### testing_open3d.py
Just me playing around with open3d







